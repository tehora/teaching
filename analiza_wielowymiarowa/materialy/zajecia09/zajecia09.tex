\documentclass{beamer}

\mode<presentation> {
  \definecolor{frameheadforeground}{RGB}{169,33,62}
  \definecolor{frameheadbackground}{RGB}{255,255,255}

  \usetheme{Warsaw}
  %\setbeamercovered{transparent}
}

\setbeamercolor{structure}{fg=frameheadforeground,bg=frameheadbackground}

\usepackage[utf8]{inputenc}
\usepackage[MeX]{polski}
\usepackage{multirow}

\begin{document}

\begin{frame}
\title[Tytuł]{Metody czynnikowe}

\author{Dorota Celińska-Kopczyńska, Paweł Strawiński}
\institute{Uniwersytet Warszawski}

\titlepage
\end{frame}
\begin{frame}[allowframebreaks]
  \tableofcontents
\end{frame}

\section{Wprowadzenie}
\begin{frame}{Wprowadzenie}
  \begin{itemize}
  \item Metody czynnikowe stanowią zbiór metod i procedur statystycznych pozwalających na redukcję dużej liczby zmiennych do kilku wzajemnie nieskorelowanych czynników.
  \item Za ich pomocą można zachować stosunkowo duża część informacji zawartych w zmiennych pierwotnych.
  \item Jednocześnie każda z tych metod niesie inne treści merytoryczne.
  \end{itemize}
\end{frame}

\section{Ogólna charakterystyka}
\subsection{Obszar zastosowań}
\begin{frame}{Cele}
  \begin{itemize}
  \item \textbf{Redukcja liczby zmiennych} bez istotnej straty zawartych w nich informacji
  \item \textbf{Transformacja} układu zmiennych w nowy układ czynników głównych
  \item \textbf{Tworzenie skal i miar} z kilku zmiennych
  \item \textbf{Ustalanie wag} określających znaczenie, jakie należy przypisać poszczególnym zmiennym podczas analizy
  \item \textbf{Ortogonalizacja przestrzeni}, w której rozpatrywane są obiekty będące przedmiotem analizy
  \item \textbf{Wykrywanie ukrytych związków} między zmiennymi
  \item \textbf{Opis zjawisk} za pomocą nowych kategorii zdefiniowanych przez czynniki
  \end{itemize}
\end{frame}

\begin{frame}{Przykłady zastosowań}
  \begin{itemize}
  \item Kiedy interesuje nas \textbf{eksploracja} i rozpoznanie struktury zbioru danych.
  \item Gdy nie posiadamy modelu ,,głębokiej'' \textbf{struktury czynników} wyjaśniających związki między danymi.
  \item Gdy potrzebujemy \textbf{zredukować zbiór zmiennych} skorelowanych ze sobą do wykorzystania ich w postaci zagregowanej w późniejszych etapach analizy.
  \item Gdy chcemy stworzyć \textbf{skalę, indeks, miernik} ukrytego zjawiska i jednoznacznie wyliczyć jego wartość. 
  \end{itemize}
\end{frame}

\begin{frame}{Przykładowe pytania i zagadnienia badawcze}
  \begin{itemize}
  \item Stworzenie indeksu kapitału społecznego (FA)
  \item Wypowiedzenie się na temat postawy respondentów w oparciu o wiele stwierdzeń dotyczących jednego zagadnienia (np. zadowolenia ze spędzania czasu wolnego) (FA lub PCA)
  \item Stworzenie agregatowej zmiennej z wartości pomiarów potrzebnej do dalszej analizy (PCA)
  \item Stworzenie zmiennej opisującej objawy depresji, do wykorzystania w regresji liniowej, celem uniknięcia silnego skorelowania zmiennych (PCA)
  \end{itemize}
\end{frame}

\subsection{Podział metod czynnikowych}
\begin{frame}{Dwa modele metod czynnikowych}
  \begin{itemize}
  \item Model \textbf{klasyczny}, w którym wariancję całkowitą zmiennych dzieli się na wariancję wspólną i wariancję specyficzną (klasyczna analiza czynnikowa).
  \item Model \textbf{komponentowy}, w którym nie uwzględnia się struktury wariancji (metoda składowych głównych).
  \end{itemize}
\end{frame}

\section{Klasyczna analiza czynnikowa}
\begin{frame}{Ogólna charakterystyka metody}
\begin{itemize}
\item Klasyczna analiza czynnikowa (\textit{factor analysis} (FA) służy do znajdowania ukrytych czynników, które określają związki pomiędzy zmiennymi pierwotnymi (obserwowalnymi).
\item Zbiór zmiennych pierwotnych dzieli się na podzbiory, które są silnie determinowane przez określoną grupę czynników (ukrytych) a słabiej przez pozostałe.
\item Jest to metoda modelowania liniowego -- zakłada się, że zmienne można przedstawić za pomocą liniowej funkcji zmiennych ukrytych (czynników)
\item Nie ma podziału na zmienne objaśniające i objaśniane.  
  \end{itemize}
\end{frame}

\begin{frame}{Obszar zastosowania analizy czynnikowej}
  \begin{enumerate}
    \item Analiza wyjaśniająca (eksploracyjna)
  \begin{itemize}
  \item Czynniki są opisywane przez grupowanie w zbiory zmiennych najsilniej ze sobą skorelowanych;
  \item Technika ma za zadanie wykryć zależności pomiędzy zmiennymi pierwotnymi, a zmiennymi ukrytymi bez wstępnych założeń dotyczących kierunku tych powiązań;
  \end{itemize}
\item Analiza potwierdzająca
  \begin{itemize}
  \item Za jej pomocą potwierdzamy hipotezy badawcze o występowaniu pewnych nieobserwowalnych zjawisk
   \item Technika ta testuje określoną strukturę czynników, w której zmienne pierwotne zależą od domniemanych lub znanych badaczowi zmiennych ukrytych
  \end{itemize}
\end{enumerate}
\end{frame}

\subsection{Formalny zapis modelu i metody estymacji}

\begin{frame}{Zależność funkcyjna}
  $$X_i = f( F_1, F_2, \dots, F_p) + \varepsilon_i $$
  \begin{itemize}
  \item $p$ -- liczba zmiennych ukrytych
  \item $k$ -- liczba zmiennych pierwotnych
  \item $X_i$ -- zmienna wyjściowa, o której zakłada się, że ma rozkład normalny $(i = 1,...,k)$
  \item $F_j$ -- zmienna ukryta, czynnik $j=1,2,...,p$, gdzie $p \leq k$
  \item $\varepsilon_i$ -- czynnik losowy, zakłada się, że jest to zmienna losowa o rozkładzie normalnym.
  \end{itemize}
\end{frame}

\begin{frame}{Zależność funkcyjna cd.}
  $$X_i = \lambda_{i1}f_1 + \lambda_{i2}f_2 + \dots + \lambda_{ip}f_{p} + \varepsilon_i $$
  $$X = \Lambda f + \varepsilon $$
  \begin{itemize}
    \item $\lambda_i$ -- waga stojąca przy $j$-tej zmiennej ukrytej dla $i$-tej zmiennej pierwotnej, inaczej ładunek czynnikowy
  \end{itemize}
\end{frame}

\begin{frame}{Założenia dotyczące wariancji}
  $$\sigma_i = \lambda^2_{i1} + \lambda^2_{i2} +\dots+ \lambda^2_{ip} + \varphi = \sum_{j=1}^p \lambda_{ij}^{2} + \varphi $$
  $$Cov(X_i, X_j) = \lambda_{i1}\lambda_{j1} +\dots + \lambda_{ip}\lambda_{jp}$$
  $$\sum = \Lambda\Lambda' + \Phi $$
  \begin{itemize}
  \item $\sigma_i$ -- wariancja zmiennej wyjściowej $X_i$
  \item $\sum_{j=1}^{p}\lambda^2_{ij}$ -- zmienność wspólna zmiennej wyjściowej $X_i$
  \item $\varphi$ -- zmienność swoista zmiennej wyjściowej $X_i$
  \item $\sum$ -- macierz kowariancji, $\Phi$ to macierz z wartościami swoistymi na przekątnej (estymowana za pomocą macierzy kowariancji lub korelacji z próby)
   \end{itemize}
\end{frame}

\begin{frame}{Założenia dodatkowe}
  \begin{itemize}
  \item Czynniki wspólne nie są skorelowane ze sobą
  \item Czynniki swoiste (inaczej specyficzne) nie są skorelowane ze sobą
  \item Czynniki wspólne i swoiste nie są ze sobą skorelowane
  \item Czynniki wspólne są zestandaryzowane: $E(F_j) = 0$ i $Var(F_{j}) = 1$
   \end{itemize}
\end{frame}

\begin{frame}{Algorytm}
  \begin{itemize}
  \item Szukamy oszacowań ładunków dla czynników oraz dla części wspólnej wariancji
  \item Po określeniu rozwiązania początkowego w następnym kroku można dokonać rotacji czynników w celu łatwiejszej interpretacji
  \item Rozwiązujemy względem $\hat{\Lambda}$ i $\hat{\Phi}$ ograniczenie:
    $$S =  \hat{\Lambda}\hat{\Lambda}' + \hat{\Phi} $$
   \end{itemize}
\end{frame}

\begin{frame}{Metoda osi głównych}
  \begin{itemize}
   \item Metodę osi głównych stosuje się przy wyznaczaniu współczynników głównych składowych
   \item Jedyna różnica, w stosunku do procedury stosowanej w analizie głównych składowych, polega na wykorzystaniu zredukowanej macierzy korelacji zamiast pełnej macierzy korelacji
   \item Na głównej przekątnej zredukowanej macierzy korelacji zamiast jedynek znajdują się wartości zasobów zmienności wspólnej kolejnych zmiennych pierwotnych
   \end{itemize}
\end{frame}

\begin{frame}{Metoda centroidalna}
  \begin{itemize}
    \begin{scriptsize}
   \item Opiera się na geometrycznym podejściu do analizy czynnikowej
   \item Kolumny macierzy danych wejściowych można interpretować jako konfigurację $m$ wektorów zmiennych w $n$ wymiarowej przestrzeni euklidesowej $R_n$. Wzajemny układ wektorów, reprezentujących zmienne, określa korelacje pomiędzy zmiennymi, tzn. cosinusy kątów między wektorami są równe współczynnikom korelacji pomiędzy zmiennymi
   \item Zakłada się, że osie poszczególnych czynników przechodzą przez środki ciężkości (centroidy) konfiguracji wektorów
   \item Kolejne czynniki wyjaśniają maksymalną część zmienności wspólnej zmiennych pierwotnych
   \item  Wartości ładunków czynnikowych to współrzędne punktów reprezentujących zmienne w nowym, ortogonalnym układzie odniesienia
    \end{scriptsize}
   \end{itemize}
\end{frame}

\begin{frame}{Metoda największej wiarygodności}
  \begin{itemize}
  \item W przeciwieństwie do innych metod, tutaj określamy liczbę czynników wspólnych, którą chcemy uzyskać \textbf{przed przystąpieniem do analizy}
  \item Założenie: dane wejściowe, zmienne wyjściowe, składnik losowy oraz funkcje zmiennych ukrytych pochodzą z próby o wielowymiarowym rozkładzie normalnym
  \item Postać funkcji wiarygodności:
    $$L = -\frac{1}{2}n \{ln|\Lambda\Lambda' + \Phi| + tr(S|\Lambda\Lambda' + \Phi|^{-1})\} $$
   \end{itemize}
\end{frame}

\subsection{Rotacja czynników}

\begin{frame}{Rotacja czynników}
  \begin{itemize}
  \item Uzyskana macierz ładunków czynnikowych nie jest jedynym możliwym rozwiązaniem analizy czynnikowej
  \item Można wygenerować nieskończenie wiele różnych macierzy ładunków poprzez obrót układu wzajemnie ortogonalnych osi
  \item Rotacja ma pomóc w znalezieniu układu, który będzie prostszy w interpretacji
  \item Istnieją dwie grupy metod rotacji: ortogonalne i ukośne
   \end{itemize}
\end{frame}

\begin{frame}{Rotacje ortogonalne}
  \begin{itemize}
  \item Polegają na znalezieniu ortogonalnej macierzy transformacji
  \item Najbardziej znane metody to \textbf{varimax} i \textbf{quartimax}
  \item Varimax minimalizuje liczbę zmiennych potrzebnych do wyjaśnienia danego czynnika
  \item Quartimax minimalizuje liczbę czynników potrzebnych do wyjaśnienia danej zmiennej
   \end{itemize}
\end{frame}

\begin{frame}{Rotacje ukośne}
  \begin{itemize}
  \item Macierz ładunków staje się macierzą wzorców zachowań
  \item Do wyznaczenia korelacji czynników wykorzystuje się wagi nadane poszczególnym czynnikom $F$
   \end{itemize}
\end{frame}

\subsection{Wybór optymalnej liczby czynników}

\begin{frame}{Metody wyboru liczby czynników}
  Do wyboru optymalnej liczby czynników można stosować następujące metody:
  \begin{itemize}
  \item Metodę procentu wariancji tłumaczonej przez czynniki
  \item Metodę wartości własnych większych od jedności
  \item Metodę testu osypiska
  \end{itemize}
  Ale i tak ostateczna decyzja jest subiektywnym wyborem badacza
\end{frame}

\begin{frame}{Metoda wartości własnej większej od jedności}
  \begin{itemize}
  \item Jest to najczęściej spotykana metoda: każdy czynnik powinien wyjaśniać zmienność co najmniej jednej zmiennej pierwotnej
  \item Polecana, jeśli liczba zmiennych jest większa niż 20
  \item W przypadku analiz na mniejszych zbiorach danych, metoda ta ma tendencję do wybierania zbyt małej liczby czynników
   \end{itemize}
\end{frame}

\begin{frame}{Metoda procentu wariancji tłumaczonej}
  \begin{itemize}
  \item Liczbę wybranych czynników ustala się na podstawie procentu wariancji przez nie tłumaczonej
  \item Dążymy do odtworzenia co najmniej 70\% wariancji (niższe wartości w przypadku dużych zbiorów danych)
  \item Żaden następny czynnik poza wybranymi przez nas nie tłumaczy więcej niż 5\% wariancji.
   \end{itemize}
\end{frame}

\begin{frame}{Metoda testu osypiska}
  \begin{itemize}
  \item Najpierw sporządzamy wykres, na którym na osi poziomej umieszczamy kolejne czynniki, natomiast na osi pionowej ich wartości własne
  \item Szukamy punktów załamania, w których zmienia się kąt załamania krzywej (zaczynają się kolejne rumowiska)
  \item Miejsce punktu załamania określa maksymalną liczbę czynników kwalifikujących się do dalszej analizy
   \item Metoda ta pozwala włączyć do analizy większą liczbę czynników niż metoda wartości własnych większych od 1
   \end{itemize}
\end{frame}

\subsection{Interpretacja i ocena jakości}

\begin{frame}{Nazwy czynników}
  \begin{itemize}
  \item Dla każdego czynnika wybieramy kilka zmiennych o najwyższych ładunkach czynnikowych
  \item Następnie spróbować nadać wspólną nazwę w oparciu o te zmienne danemu czynnikowi
  \item Mając nazwy czynników spróbować znaleźć ich odniesienie do danego, głębszego wymiaru, ich związek z badaną zmienną ukrytą
   \end{itemize}
\end{frame}

\begin{frame}{Wskaźnik Kaisera-Mayera-Olkina (KMO)}
  \begin{itemize}
  \item Informuje, czy istnieją podstawy do stosowania analizy czynnikowej
  \item Indeks o wartościach [0,1] porównuje cząstkowe współczynniki korelacji z dwuzmiennowymi współczynnikami korelacji
    $$KMO = \frac{\sum_{i \ne j}\sum_{j \ne i} r_{ij}^2}{\sum_{i \ne j}\sum_{j \ne i}r^2_{ij} + \sum_{i \ne j}\sum_{j \ne i}a^2_{ij} }$$
  \item $r_{ij}$ -- element macierzy korelacji R
  \item $a_{ij}$ -- współczynnik korelacji cząstkowej
  \item Im wartość wskaźnika bliższa 1, tym silniejsze podstawy do zastosowania analizy czynnikowej
   \end{itemize}
\end{frame}

\begin{frame}{Test Bartletta o sferyczności}
  \begin{itemize}
  \item $H_0$: $R = I$ (macierz korelacji jest macierzą jednostkową)
  \item $H_1$: $R \ne I$ (macierz korelacji nie jest macierzą jednostkową)
  \item Dążymy do odrzucenia $H_0$
  \end{itemize}
\end{frame}


\section{Analiza składowych głównych -- PCA}

\subsection{Ogólna charakterystyka}
\begin{frame}{Analiza składowych głównych (PCA)}
  \begin{itemize}
  \item Stanowi metodę transformacji zmiennych pierwotnych we wzajemnie ortogonalne nowe zmienne, tzw. składowe główne
  \item Służy redukcji wymiaru przestrzeni cech oraz pogrupowaniu ich w podzbiory
  \item Dzięki niej można graficznie zaprezentować konfigurację porównywanych zmiennych
   \end{itemize}
\end{frame}

\begin{frame}{Ogólna charakterystyka -- cd.}
  \begin{itemize}
  \item Zmienne pierwotne poddaje się standaryzacji, więc ich wariancje są sobie równe
  \item Nowa agregatowa zmienna powinna wyjaśniać maksymalną ilość wariancji zmiennych pierwotnych
  \item Wariancja nowej zmiennej agregatowej jest nazywana wartością własną (\emph{eigenvalue})
   \item Zbiór danych powinien być jednorodny (brak obserwacji odstających)
   \end{itemize}
\end{frame}

\subsection{Metoda}
\begin{frame}{Zapis formalny modelu}
  $$PC_i = w_{i1}X_1 + w_{i2}X_2 + \dots + w_{ik}X_k $$
  $$\sum_{j=1}^{k}w_{ij}^{2} = 1 $$
  \begin{itemize}
  \item Współczynniki $w$ przy zmiennych $X$ stanowią wagi, jakie przypisuje się zmiennym w tworzeniu głównej składowej
  \item Zakładamy, że poszukiwane czynniki są niezależne i mają wystandaryzowany rozkład normalny
  \end{itemize}
\end{frame}

\begin{frame}{Wyznaczanie współczynników}
  \begin{itemize}
  \item Wartości wektora $w$ są tak dobierane, żeby maksymalizować wariancję $PC$
  \item Szukamy wartości własnych następującego równania:
    $$ |R - \lambda I| = 0$$
  \item $R$ -- macierz korelacji $k$ zmiennych wyjściowych
  \item $\Lambda$ -- wektor zawierający wartości własne o wymiarach $kxk$
  \item Wariancją $i$-tej składowej jest $i$-ta wartość własna
  \end{itemize}
\end{frame}

\begin{frame}{Wyznaczanie współczynników -- cd}
  \begin{itemize}
  \item Każdej wartości własnej możemy przypisać wektor własny macierzy o postaci:
    $$Rw_i = \lambda_iw_i $$
  \item $w_i$ -- wektor własny macierzy korelacji
  \item Wartości składowe tego wektora stanowią wartości współczynników stojących przy zmiennych pierwotnych; ich kombinacja tworzy nowe zmienne: składowe główne
   \item Pułapka: utworzona kombinacja liniowa jest zależna od jednostek miary i rzędów wielkości poszczególnych zmiennych (należy standaryzować zmienne!)
  \end{itemize}
\end{frame}

\begin{frame}{Wybór optymalnej liczby składowych głównych}
  \begin{itemize}
  \item Dążymy do odtworzenia maksymalnej ilości informacji z pierwotnego zbioru zmiennych
  \item W praktyce wybieramy liczbę składowych, które łącznie wyjaśniają powyżej 70\% zmienności zmiennych pierwotnych
  \item Nie uwzględniamy tych składowych, dla których wartości własne są niższe od średniej
  \item Można opuścić główne składowe, dla których wartości własne są niższe od 1 (symulacje wskazują, że lepszym progiem jest 0,7)
  \item Opuszczamy składowe, które mają mniejszy udział w wariancji niż 5\%
   \end{itemize}
\end{frame}

\section{Porównanie FA i PCA}
\begin{frame}{Wybór właściwego modelu}
  \begin{itemize}
  \item Wybór między PCA a FA zależy przede wszystkim od celu analizy
  \item W klasycznej analizie czynnikowej mała liczba czynników pozwala wyjaśniać zależności pomiędzy zmiennymi obserwowalnymi; chcemy zidentyfikować zmienne ukryte
  \item W analizie składowych głównych dążymy do zachowania jak największej ilości informacji przy jak najmniejszej liczbie nowych zmiennych; chcemy uprościć strukturę danych
  \item FA to analiza modelowa, PCA to technika eksploracyjna, pomocnicza
  \end{itemize}
\end{frame}

\begin{frame}{FA i PCA -- różnice}
  \begin{itemize}
  \item \textbf{Wariancja} -- FA obejmuje pewną część wariancji, zwaną wariancją wspólną; PCA obejmuje wariancję całkowitą zmiennych
  \item \textbf{Punkt wyjścia} -- dla FA to zredukowana macierz korelacji; dla PCA zwykła macierz korelacji
  \item \textbf{Zmienne pierwotne} -- w FA zmienna pierwotna jest funkcją czynników wspólnych i swoistych; w PCA główna składowa jest funkcją zmiennych pierwotnych
  \item \textbf{Zależność} -- w FA czynniki mogą być skorelowane; w PCA główne składowe są zawsze niezależne
   \end{itemize}
\end{frame}

\end{document}
